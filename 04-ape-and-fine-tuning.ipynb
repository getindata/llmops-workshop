{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "1. âœ… Remember to change the Colab runtime type to **GPU T4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Clone repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env EXERCISE_NAME=04-ape-and-fine-tuning\n",
    "%env REPO_NAME=llmops-workshop\n",
    "%env BRANCH_NAME=workshop-prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Cloning the repository...${REPO_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/getindata/$REPO_NAME.git 2> /dev/null || cd $REPO_NAME && git checkout $BRANCH_NAME && git stash && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Tools versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env LLAMA_CPP_VERSION=b5043\n",
    "%env OLLAMA_VERSION=0.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab  # noqa: F401\n",
    "\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Prepare environment variables, Python modules search path and install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if is_colab():\n",
    "    os.environ[\"EXERCISE_ROOT\"] = f\"/content/{os.environ['REPO_NAME']}/exercises\"\n",
    "    os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/lib64-nvidia:/bin/\"\n",
    "    os.environ[\"EXERCISE_RUN\"] = \"colab\"\n",
    "else:\n",
    "    os.environ[\"EXERCISE_ROOT\"] = f\"{os.getcwd()}/exercises\"\n",
    "    os.environ[\"EXERCISE_RUN\"] = \"local\"\n",
    "\n",
    "os.environ['EXERCISE_DIR'] = f\"{os.environ['EXERCISE_ROOT']}/{os.environ['EXERCISE_NAME']}\"\n",
    "get_ipython().run_line_magic(\"run\", f\"{os.environ['EXERCISE_ROOT']}/shared/setup/__setup_{os.environ['EXERCISE_RUN']}.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f\"{os.environ['EXERCISE_DIR']}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r {os.environ['EXERCISE_DIR']}/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!llama-cli --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ollama pull gemma3:1b > /dev/null 2>&1\n",
    "curl -sX POST http://localhost:11434/api/generate -d '{\n",
    "\"model\": \"gemma3:1b\",\n",
    "\"system\": \"Be helpful and concise.\",\n",
    "\"prompt\":\"Who discovered general theory of relativity?.Just answer with the name of the scientist.\"\n",
    "}' | jq -r '. | {response} | join(\",\")' | tr -d '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Langfuse connection (also OpenTelemetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata  # noqa: F401\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = (\n",
    "    getpass.getpass(\"LANGFUSE_SECRET_KEY\")\n",
    "    if not is_colab()\n",
    "    else userdata.get(\"LANGFUSE_SECRET_KEY\")\n",
    ")\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = (\n",
    "    getpass.getpass(\"LANGFUSE_PUBLIC_KEY\")\n",
    "    if not is_colab()\n",
    "    else userdata.get(\"LANGFUSE_PUBLIC_KEY\")\n",
    ")\n",
    "os.environ[\"LANGFUSE_HOST\"] = (\n",
    "    getpass.getpass(\"LANGFUSE_HOST\")\n",
    "    if not is_colab()\n",
    "    else userdata.get(\"LANGFUSE_HOST\")\n",
    ")\n",
    "\n",
    "os.environ[\"LANGFUSE_AUTH\"] = base64.b64encode(\n",
    "    f\"{os.environ['LANGFUSE_PUBLIC_KEY']}:{os.environ['LANGFUSE_SECRET_KEY']}\".encode()\n",
    ").decode()\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\"] = (\n",
    "    f\"{os.environ['LANGFUSE_HOST']}/api/public/otel/v1/traces\"\n",
    ")\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_TRACES_HEADERS\"] = (\n",
    "    f\"Authorization=Basic {os.environ['LANGFUSE_AUTH']}\"\n",
    ")\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_TRACES_PROTOCOL\"] = \"http/protobuf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "assert langfuse.auth_check()\n",
    "print(f\"Langfuse access works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(f\"{os.environ['EXERCISE_DIR']}/data/input/sms_phishing.csv\")\n",
    "datasets = []\n",
    "for r in df.iter_rows(named=True):\n",
    "    datasets.append({\"input\": r[\"TEXT\"], \"expected_output\": r[\"LABEL\"].lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PCT = 0.9666\n",
    "VAL_PCT = 0.0167\n",
    "TEST_PCT = 0.0167\n",
    "train_ds = datasets[: int(len(datasets) * TRAIN_PCT)]\n",
    "val_ds = datasets[\n",
    "    int(len(datasets) * TRAIN_PCT) : int(len(datasets) * (TRAIN_PCT + VAL_PCT))\n",
    "]\n",
    "test_ds = datasets[int(len(datasets) * (TRAIN_PCT + VAL_PCT)) :]\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Create Langfuse datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(name: str, ds: list):\n",
    "    langfuse.create_dataset(name=name)\n",
    "    for item in ds:\n",
    "        langfuse.create_dataset_item(\n",
    "            dataset_name=name,\n",
    "            input=item[\"input\"],\n",
    "            expected_output=item[\"expected_output\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(\"sms_phishing_train\", train_ds)\n",
    "create_dataset(\"sms_phishing_val\", val_ds)\n",
    "create_dataset(\"sms_phishing_test\", test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Register the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_prompt(\n",
    "    name=\"sms-classifier\",\n",
    "    type=\"text\",\n",
    "    prompt=\" Given an SMS text, classify whether it is 'ham', 'spam', or 'smishing'. Output only the predicted category.\",\n",
    "    labels=[\"sandbox\"],\n",
    "    tags=[\"sms\", \"dspy\", \"classifier\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Create a SMSClassifier module with DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Configure which LLM to use (Ollama Gemma3)\n",
    "DSpy uses [LiteLLM](https://www.litellm.ai/) to abstract the LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "MODEL = \"gemma3:1b\"\n",
    "\n",
    "lm = dspy.LM(f\"ollama/{MODEL}\", cache=False, num_retries=5)\n",
    "dspy.settings.configure(lm=lm, track_usage=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Configuring DSPy traces with OpenTelemetry using MLflow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"mlflow\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Create a DSPy\n",
    "1. We first create a Signature that defines the schema of the input and output of the module and well as the default instructions.\n",
    "2. Then we create a Module that combines the Signature with the prompting strategy, such Chain of Thought, ReAct or Predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import dspy\n",
    "\n",
    "\n",
    "class SMSClassifierSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Given an SMS text, predict whether it is ham, spam, or smishing.\n",
    "    Output only the predicted label.\n",
    "    \"\"\"\n",
    "\n",
    "    sms_text: str = dspy.InputField()\n",
    "    category: Literal[\"ham\", \"spam\", \"smishing\"] = dspy.OutputField()\n",
    "\n",
    "\n",
    "class SMSClassifier(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(SMSClassifierSignature)\n",
    "\n",
    "    @mlflow.trace\n",
    "    def forward(self, sms_text):\n",
    "        return self.generate_answer(sms_text=sms_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier = SMSClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Change the default instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMSClassifierSignature.__doc__ = langfuse.get_prompt(\n",
    "    name=\"sms-classifier\", version=2\n",
    ").compile()\n",
    "sms_classifier = SMSClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier(sms_text=\"Sorry i missed your call. Can you please call back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_classifier(\n",
    "    sms_text=\"We have received a request from you for purchasing Paytm Payments Bank FASTag. Total amount to be paid is Rs 250/- (Tag Cost Rs. 100 Security Deposit Rs. 0, Threshold -Rs. 150). In case of any queries, please contact us at https://m.paytm.me/fastag-help\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong should be categorized as spam\n",
    "sms_classifier(\n",
    "    sms_text=\"Rental  Helpline  9999991155 1000-100, 000 sq.ft. In Noida & Gr.Noida  OFFICE & INDUSTRIAL Use Goldenberg estates pvt.ltd. sandeep@goldenberg.in 999 999 115\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Run evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Pull the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "models=(\"gemma3:1b\" \"qwen2.5:1.5b\" \"qwen2.5:14b\" \"gemma3:12b\")\n",
    "for model in \"${models[@]}\"; do\n",
    "    echo \"Pulling model: $model\"\n",
    "    ollama pull \"$model\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Prepare the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(\"sms_phishing_test\")\n",
    "from dspy import Example\n",
    "\n",
    "test_ds = []\n",
    "for item in dataset.items:\n",
    "    test_ds.append(\n",
    "        Example(sms_text=item.input, category=item.expected_output).with_inputs(\n",
    "            \"sms_text\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from evaluation_helpers import plot_metrics, run_evaluation\n",
    "\n",
    "baseline_run_id = \"uuid.uuid1()\"\n",
    "\n",
    "models = [\n",
    "    \"gemma3:1b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"qwen2.5:14b\",\n",
    "    \"gemma3:12b\",\n",
    "]\n",
    "\n",
    "\n",
    "# full run (if you would like to run all models - remember to pull them first)\n",
    "# models = [\n",
    "#         \"smollm2:360m\", \"smollm2:1.7b\",\n",
    "#         \"qwen2.5:0.5b\", \"qwen2.5:1.5b\",\n",
    "#         \"gemma3:1b\", \"gemma3:4b\", \"granite3.2:2b\",\n",
    "#         \"gemma3:12b\", \"qwen2.5:14b\", \"phi4:14b\"\n",
    "# ]\n",
    "\n",
    "classes = [\"ham\", \"spam\", \"smishing\"]\n",
    "\n",
    "run_evaluation(\n",
    "    models, sms_classifier, test_ds, classes, baseline_run_id, prefix=\"baseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_helpers import get_all_metric\n",
    "\n",
    "metrics_baseline = get_all_metric(models, baseline_run_id, classes, prefix=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\n",
    "    metrics_baseline,\n",
    "    [\"Precision (macro)\", \"Recall(macro)\", \"F1(macro)\"],\n",
    "    \"Scores by model before prompt optimization and fine-tuning\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Automatic prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Prepare the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from dspy import Example\n",
    "\n",
    "dataset = langfuse.get_dataset(\"sms_phishing_train\")\n",
    "\n",
    "sample_size = 100\n",
    "category_split = {\"ham\": 0.6, \"spam\": 0.2, \"smishing\": 0.2}\n",
    "sample_counts = {\"ham\": 0, \"spam\": 0, \"smishing\": 0}\n",
    "ham_num = sample_size * category_split[\"ham\"]\n",
    "spam_num = sample_size * category_split[\"spam\"]\n",
    "smishing_num = sample_size * category_split[\"smishing\"]\n",
    "train_ds = []\n",
    "for item in dataset.items:\n",
    "    train_ds.append(\n",
    "        Example(sms_text=item.input, category=item.expected_output).with_inputs(\n",
    "            \"sms_text\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "categories = [item.category for item in train_ds[:sample_size]]\n",
    "Counter(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(f\"ollama/{MODEL}\", cache=False)\n",
    "dspy.settings.configure(lm=lm, track_usage=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "from evaluation_helpers import validate_answer\n",
    "\n",
    "config = dict(\n",
    "    max_bootstrapped_demos=2,\n",
    "    max_labeled_demos=8,\n",
    "    num_candidate_programs=4,\n",
    "    num_threads=1,\n",
    "    max_errors=10,\n",
    ")\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=validate_answer, **config)\n",
    "optimized_program = teleprompter.compile(\n",
    "    sms_classifier, trainset=train_ds[:sample_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program.save(\n",
    "    f\"{os.environ['EXERCISE_DIR']}/programs/sms_classifier-{MODEL}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_classifier = SMSClassifier()\n",
    "optimized_classifier.load(\n",
    "    f\"{os.environ['EXERCISE_DIR']}/programs/sms_classifier-{MODEL}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(\"sms_phishing_test\")\n",
    "from dspy import Example\n",
    "\n",
    "test_ds = []\n",
    "for item in dataset.items:\n",
    "    test_ds.append(\n",
    "        Example(sms_text=item.input, category=item.expected_output).with_inputs(\n",
    "            \"sms_text\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [MODEL]\n",
    "classes = [\"ham\", \"spam\", \"smishing\"]\n",
    "ape_run_id = uuid.uuid1()\n",
    "run_evaluation(models, optimized_classifier, test_ds, classes, ape_run_id, prefix=\"ape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ape = get_all_metric(models, ape_run_id, classes, prefix=\"ape\")\n",
    "metrics_ape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_classifier = SMSClassifier()\n",
    "optimized_classifier.load(\n",
    "    f\"{os.environ['EXERCISE_DIR']}/programs/sms_classifier-gemma3:1b-100ex-16p.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_evaluation(\n",
    "    models, optimized_classifier, test_ds, classes, ape_run_id, prefix=\"ape-100ex-16p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ape_16p = get_all_metric(models, ape_run_id, classes, prefix=\"ape-100ex-16p\")\n",
    "metrics_ape_16p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_baseline[\"gemma3:1b-ape\"] = metrics_ape[\"gemma3:1b\"]\n",
    "metrics_baseline[\"gemma3:1b-ape-100ex-16p\"] = metrics_ape_16p[\"gemma3:1b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\n",
    "    metrics_baseline,\n",
    "    [\"Precision (macro)\", \"Recall(macro)\", \"F1(macro)\"],\n",
    "    \"Scores by model before and after prompt optimization \",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### Langfuse evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Fine-tuning using LoRA method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "system_content_str = \"\"\"\n",
    "    Your input fields are:\n",
    "    1. `sms_text` (str)\n",
    "\n",
    "    Your output fields are:\n",
    "    1. `category` (Literal['ham', 'spam', 'smishing'])\n",
    "\n",
    "    All interactions will be structured in the following way, with the appropriate values filled in.\n",
    "\n",
    "    Inputs will have the following structure:\n",
    "\n",
    "    [[ ## sms_text ## ]]\n",
    "    {sms_text}\n",
    "\n",
    "    Outputs will be a JSON object with the following fields.\n",
    "\n",
    "    {\n",
    "      \"category\": \"{category}        # note: the value you produce must be one of: ham; spam; smishing\"\n",
    "    }\n",
    "\n",
    "    In adhering to this structure, your objective is:\n",
    "            Given an SMS text, predict whether it is ham, spam, or smishing.\n",
    "            Output only the predicted label.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "user_content_str = \"\"\"\n",
    "    [ ## sms_text ## ]]\n",
    "    {{ sms_text }}\n",
    "\n",
    "    Respond with a JSON object in the following order of fields: `category` (must be formatted as a valid Python Literal['ham', 'spam', 'smishing']).\n",
    "\"\"\"\n",
    "\n",
    "predict_str = \"\"\"\n",
    "```json\n",
    "{\n",
    "  \"category\": {{ predicted_label | tojson }}\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template_chat_str = \"\"\"\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": {{ system_prompt | tojson }}\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": {{ user_prompt | tojson}}\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": {{ predict | tojson }}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "template_complete_str = \"\"\"\n",
    "{\n",
    "  \"prompt\": \"Given an SMS text, predict whether it is ham, spam, or smishing.Output only the predicted label: {{ sms_text }}\",\n",
    "  \"completion\": \"{{ predicted_label }}\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "system_template = Template(system_content_str)\n",
    "user_template = Template(user_content_str)\n",
    "predict_template = Template(predict_str)\n",
    "template_chat = Template(template_chat_str)\n",
    "dataset = langfuse.get_dataset(\"sms_phishing_train\")\n",
    "data = []\n",
    "for item in dataset.items:\n",
    "    data.append(\n",
    "        template_chat.render(\n",
    "            system_prompt=system_template.render(\n",
    "                sms_text=item.input, category=item.expected_output\n",
    "            ),\n",
    "            user_prompt=user_template.render(sms_text=item.input),\n",
    "            predict=predict_template.render(predicted_label=item.expected_output),\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(f\"{os.environ['EXERCISE_DIR']}/data/train/train.jsonl\", \"w\") as f:\n",
    "    for record in data:\n",
    "        try:\n",
    "            parsed = json.loads(record)\n",
    "            json_record = json.dumps(parsed)\n",
    "            f.write(json_record + \"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(\"sms_phishing_val\")\n",
    "data = []\n",
    "for item in dataset.items:\n",
    "    data.append(\n",
    "        template_chat.render(\n",
    "            system_prompt=system_template.render(\n",
    "                sms_text=item.input, category=item.expected_output\n",
    "            ),\n",
    "            user_prompt=user_template.render(sms_text=item.input),\n",
    "            predict=predict_template.render(predicted_label=item.expected_output),\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(f\"{os.environ['EXERCISE_DIR']}/data/train/valid.jsonl\", \"w\") as f:\n",
    "    for record in data:\n",
    "        try:\n",
    "            parsed = json.loads(record)\n",
    "            json_record = json.dumps(parsed)\n",
    "            f.write(json_record + \"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama pull getindata/gemma3:1b-it-llmops-q4_K_M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"getindata/gemma3:1b-it-llmops-q4_K_M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_run_id = uuid.uuid1()\n",
    "models = [model]\n",
    "sms_classifier = SMSClassifier()\n",
    "run_evaluation(models, sms_classifier, test_ds, classes, ft_run_id, prefix=\"fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ft = get_all_metric(models, ft_run_id, classes, prefix=\"fine-tune\")\n",
    "metrics_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_baseline[\"gemma3:1b-ft\"] = metrics_ft[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\n",
    "    metrics_baseline,\n",
    "    [\"Precision (macro)\", \"Recall(macro)\", \"F1(macro)\"],\n",
    "    \"Scores by model before and after prompt optimization and fine-tuning\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
