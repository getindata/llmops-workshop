{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T14:21:11.472744Z",
     "start_time": "2025-03-30T14:21:11.466767Z"
    }
   },
   "source": [
    "## LOCAL\n",
    "%run __include.ipynb"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:21:12.594059Z",
     "start_time": "2025-03-30T14:21:12.579437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()"
   ],
   "id": "b9b4c571cd7f937b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:49:12.725479Z",
     "start_time": "2025-04-01T04:49:12.721837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "system_content_str = \"\"\"\n",
    "    Your input fields are:\n",
    "    1. `sms_text` (str)\n",
    "\n",
    "    Your output fields are:\n",
    "    1. `category` (Literal['ham', 'spam', 'smishing'])\n",
    "\n",
    "    All interactions will be structured in the following way, with the appropriate values filled in.\n",
    "\n",
    "    Inputs will have the following structure:\n",
    "\n",
    "    [[ ## sms_text ## ]]\n",
    "    {sms_text}\n",
    "\n",
    "    Outputs will be a JSON object with the following fields.\n",
    "\n",
    "    {\n",
    "      \"category\": \"{category}        # note: the value you produce must be one of: ham; spam; smishing\"\n",
    "    }\n",
    "\n",
    "    In adhering to this structure, your objective is:\n",
    "            Given an SMS text, predict whether it is ham, spam, or smishing.\n",
    "            Output only the predicted label.\n",
    "        \"\"\"\n",
    "\n",
    "# system_content_str = \"Given an SMS text, predict whether it is ham, spam, or smishing.Output only the predicted label.\"\n",
    "\n",
    "user_content_str = \"\"\"\n",
    "    [ ## sms_text ## ]]\n",
    "    {{ sms_text }}\n",
    "\n",
    "    Respond with a JSON object in the following order of fields: `category` (must be formatted as a valid Python Literal['ham', 'spam', 'smishing']).\n",
    "\"\"\"\n",
    "\n",
    "predict_str = \"\"\"\n",
    "```json\n",
    "{\n",
    "  \"category\": {{ predicted_label | tojson }}\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template_chat_str = \"\"\"\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": {{ system_prompt | tojson }}\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": {{ user_prompt | tojson}}\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": {{ predict | tojson }}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "template_complete_str = \"\"\"\n",
    "{\n",
    "  \"prompt\": \"Given an SMS text, predict whether it is ham, spam, or smishing.Output only the predicted label: {{ sms_text }}\",\n",
    "  \"completion\": \"{{ predicted_label }}\"\n",
    "}\n",
    "\"\"\""
   ],
   "id": "cbbae7d1eb56f81c",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:49:16.742582Z",
     "start_time": "2025-04-01T04:49:15.005256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "system_template = Template(system_content_str)\n",
    "user_template = Template(user_content_str)\n",
    "predict_template = Template(predict_str)\n",
    "template_chat = Template(template_chat_str)\n",
    "dataset = langfuse.get_dataset(\"sms_phishing_train\")\n",
    "print(template_chat_str)\n",
    "data = []\n",
    "for item in dataset.items:\n",
    "    data.append(\n",
    "        template_chat.render(\n",
    "            system_prompt=system_template.render(\n",
    "                sms_text=item.input, category=item.expected_output\n",
    "            ),\n",
    "            user_prompt=user_template.render(sms_text=item.input),\n",
    "            predict=predict_template.render(predicted_label=item.expected_output),\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(\"../../mlx/data/train.jsonl\", \"w\") as f:\n",
    "    for record in data:\n",
    "        try:\n",
    "            parsed = json.loads(record)\n",
    "            json_record = json.dumps(parsed)\n",
    "            f.write(json_record + \"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass"
   ],
   "id": "f098a9b331a5c6ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": {{ system_prompt | tojson }}\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": {{ user_prompt | tojson}}\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": {{ predict | tojson }}\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:49:20.447549Z",
     "start_time": "2025-04-01T04:49:20.395854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = langfuse.get_dataset(\"sms_phishing_val\")\n",
    "data = []\n",
    "for item in dataset.items:\n",
    "    data.append(\n",
    "        template_chat.render(\n",
    "            system_prompt=system_template.render(\n",
    "                sms_text=item.input, category=item.expected_output\n",
    "            ),\n",
    "            user_prompt=user_template.render(sms_text=item.input),\n",
    "            predict=predict_template.render(predicted_label=item.expected_output),\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(\"../../mlx/data/valid.jsonl\", \"w\") as f:\n",
    "    for record in data:\n",
    "        try:\n",
    "            parsed = json.loads(record)\n",
    "            json_record = json.dumps(parsed)\n",
    "            f.write(json_record + \"\\n\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass"
   ],
   "id": "1916fd1b1ebdae7c",
   "outputs": [],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:03:33.664942Z",
     "start_time": "2025-03-30T14:03:31.939848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! mlx_lm.lora --help"
   ],
   "id": "d30eda4cfef14b79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: mlx_lm.lora [-h] [--model MODEL] [--train] [--data DATA]\r\n",
      "                   [--fine-tune-type {lora,dora,full}]\r\n",
      "                   [--optimizer {adam,adamw}] [--mask-prompt]\r\n",
      "                   [--num-layers NUM_LAYERS] [--batch-size BATCH_SIZE]\r\n",
      "                   [--iters ITERS] [--val-batches VAL_BATCHES]\r\n",
      "                   [--learning-rate LEARNING_RATE]\r\n",
      "                   [--steps-per-report STEPS_PER_REPORT]\r\n",
      "                   [--steps-per-eval STEPS_PER_EVAL]\r\n",
      "                   [--resume-adapter-file RESUME_ADAPTER_FILE]\r\n",
      "                   [--adapter-path ADAPTER_PATH] [--save-every SAVE_EVERY]\r\n",
      "                   [--test] [--test-batches TEST_BATCHES]\r\n",
      "                   [--max-seq-length MAX_SEQ_LENGTH] [-c CONFIG]\r\n",
      "                   [--grad-checkpoint] [--seed SEED]\r\n",
      "\r\n",
      "LoRA or QLoRA finetuning.\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\r\n",
      "                        repo.\r\n",
      "  --train               Do training\r\n",
      "  --data DATA           Directory with {train, valid, test}.jsonl files or the\r\n",
      "                        name of a Hugging Face dataset (e.g., 'mlx-\r\n",
      "                        community/wikisql')\r\n",
      "  --fine-tune-type {lora,dora,full}\r\n",
      "                        Type of fine-tuning to perform: lora, dora, or full.\r\n",
      "  --optimizer {adam,adamw}\r\n",
      "                        Optimizer to use for training: adam or adamw\r\n",
      "  --mask-prompt         Mask the prompt in the loss when training\r\n",
      "  --num-layers NUM_LAYERS\r\n",
      "                        Number of layers to fine-tune. Default is 16, use -1\r\n",
      "                        for all.\r\n",
      "  --batch-size BATCH_SIZE\r\n",
      "                        Minibatch size.\r\n",
      "  --iters ITERS         Iterations to train for.\r\n",
      "  --val-batches VAL_BATCHES\r\n",
      "                        Number of validation batches, -1 uses the entire\r\n",
      "                        validation set.\r\n",
      "  --learning-rate LEARNING_RATE\r\n",
      "                        Adam learning rate.\r\n",
      "  --steps-per-report STEPS_PER_REPORT\r\n",
      "                        Number of training steps between loss reporting.\r\n",
      "  --steps-per-eval STEPS_PER_EVAL\r\n",
      "                        Number of training steps between validations.\r\n",
      "  --resume-adapter-file RESUME_ADAPTER_FILE\r\n",
      "                        Load path to resume training from the given fine-tuned\r\n",
      "                        weights.\r\n",
      "  --adapter-path ADAPTER_PATH\r\n",
      "                        Save/load path for the fine-tuned weights.\r\n",
      "  --save-every SAVE_EVERY\r\n",
      "                        Save the model every N iterations.\r\n",
      "  --test                Evaluate on the test set after training\r\n",
      "  --test-batches TEST_BATCHES\r\n",
      "                        Number of test set batches, -1 uses the entire test\r\n",
      "                        set.\r\n",
      "  --max-seq-length MAX_SEQ_LENGTH\r\n",
      "                        Maximum sequence length.\r\n",
      "  -c CONFIG, --config CONFIG\r\n",
      "                        A YAML configuration file with the training options\r\n",
      "  --grad-checkpoint     Use gradient checkpointing to reduce memory use.\r\n",
      "  --seed SEED           The PRNG seed\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T16:37:38.811538Z",
     "start_time": "2025-03-30T16:37:36.981311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! mlx_lm.fuse --help"
   ],
   "id": "5caf08ff7be9a00a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\r\n",
      "usage: mlx_lm.fuse [-h] [--model MODEL] [--save-path SAVE_PATH]\r\n",
      "                   [--adapter-path ADAPTER_PATH] [--hf-path HF_PATH]\r\n",
      "                   [--upload-repo UPLOAD_REPO] [--de-quantize] [--export-gguf]\r\n",
      "                   [--gguf-path GGUF_PATH]\r\n",
      "\r\n",
      "Fuse fine-tuned adapters into the base model.\r\n",
      "\r\n",
      "options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --model MODEL         The path to the local model directory or Hugging Face\r\n",
      "                        repo.\r\n",
      "  --save-path SAVE_PATH\r\n",
      "                        The path to save the fused model.\r\n",
      "  --adapter-path ADAPTER_PATH\r\n",
      "                        Path to the trained adapter weights and config.\r\n",
      "  --hf-path HF_PATH     Path to the original Hugging Face model. Required for\r\n",
      "                        upload if --model is a local directory.\r\n",
      "  --upload-repo UPLOAD_REPO\r\n",
      "                        The Hugging Face repo to upload the model to.\r\n",
      "  --de-quantize         Generate a de-quantized model.\r\n",
      "  --export-gguf         Export model weights in GGUF format.\r\n",
      "  --gguf-path GGUF_PATH\r\n",
      "                        Path to save the exported GGUF format model weights.\r\n",
      "                        Default is ggml-model-f16.gguf.\r\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T10:36:57.053798Z",
     "start_time": "2025-04-01T10:36:52.983255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!  mlx_lm.fuse --model mlx-community/gemma-3-1b-it-bf16 --adapter-path ../../mlx/adapters-merge --save-path ../../mlx/models/gemma-3-1b-it-bf16-ft"
   ],
   "id": "8e394471352c11b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\r\n",
      "Fetching 9 files: 100%|████████████████████████| 9/9 [00:00<00:00, 34223.70it/s]\r\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T04:16:07.674074Z",
     "start_time": "2025-04-01T04:15:51.657250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!  mlx_lm.fuse --model mlx-community/gemma-3-4b-it-bf16 --adapter-path ../../mlx/adapters --save-path ../../mlx/models/gemma-3-4b-it-16bit-ft-8-8-5k"
   ],
   "id": "e71d47d3188e61e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\r\n",
      "Fetching 11 files: 100%|█████████████████████| 11/11 [00:00<00:00, 54729.95it/s]\r\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T08:49:36.199784Z",
     "start_time": "2025-03-31T08:49:35.434364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! convert_hf_to_gguf.py models/gemma-3-1b-it-16bit-ft-8-8-5k  --outfile models/gguf/gemma-3-1b-it-bf16-ft-8-8-5k.gguf"
   ],
   "id": "e0d34d48ef305fe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/homebrew/bin/convert_hf_to_gguf.py\", line 21, in <module>\r\n",
      "    import numpy as np\r\n",
      "ModuleNotFoundError: No module named 'numpy'\r\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!  ollama create gemmma3:1b-it-bf16_ft -f Modelfile"
   ],
   "id": "2f381dd9ab0bf8a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
