{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Playground #3 - Monitoring, evaluation, app deployment."
   ],
   "metadata": {
    "id": "YI4O30zXVsuM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Dependencies installation\n",
    "!pip install langchain langchain-community langchain-openai tiktoken langfuse lancedb langgraph docling langchain-docling gradio python-dotenv"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxfPyVTcVpWE",
    "outputId": "c305a139-0bf3-498c-8847-eaa03e9dc0a7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "# Set environment variables from secrets\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get(\"LANGFUSE_SECRET_KEY\")\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get(\"LANGFUSE_PUBLIC_KEY\")\n",
    "os.environ[\"LANGFUSE_HOST\"] = userdata.get(\"LANGFUSE_HOST\")"
   ],
   "metadata": {
    "id": "GUJ3JsHgWJzV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!gsutil cp -r gs://gid-rag-platform-llmops-workshops/dummy.json sa_key.json",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEGmuMGfenpy",
    "outputId": "cfd9af13-d046-4ddd-d4f8-79ab50f6ea0c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. App Setup"
   ],
   "metadata": {
    "id": "C6KxJWRDW6Fl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# access existing knowledge bases\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import LanceDB\n",
    "import lancedb\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"sa_key.json\"\n",
    "\n",
    "KNOWLEDGE_BASES= {\n",
    "  \"WIZARD_OF_OZ\": \"gs://gid-rag-platform-llmops-workshops/wizard_of_oz\",\n",
    "  \"ODDYSEY\": \"gs://gid-rag-platform-llmops-workshops/oddysey\",\n",
    "  \"PRAWO_O_RUCHU_DROGOWYM\": \"gs://gid-rag-platform-llmops-workshops/prawo_o_ruchu_drogowym\"\n",
    "}\n",
    "\n",
    "def get_vector_store_by_name(name: str):\n",
    "  connection = lancedb.connect(KNOWLEDGE_BASES['ODDYSEY'])\n",
    "  return LanceDB(\n",
    "      connection=connection,\n",
    "      embedding=OpenAIEmbeddings(),\n",
    "      table_name=\"knowledge_base\"\n",
    "  )\n",
    "\n"
   ],
   "metadata": {
    "id": "gHRq2YjA5x7E"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# play around with vector_store\n",
    "get_vector_store_by_name('ODDYSEY').similarity_search(\"Who was Calypso?\", k=5)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9BByOXj7dkG",
    "outputId": "7434faf3-975a-4e29-b599-83ea044888aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define graph state\n",
    "class RAGState(TypedDict):\n",
    "  knowledge_base: str\n",
    "  no_snippets: int\n",
    "  question: str\n",
    "  snippets: list\n",
    "  answer: str\n",
    "\n",
    "\n",
    "# Node #1 - retrieval\n",
    "def retrieve_snippets(state: RAGState):\n",
    "  no_snippets = state.get(\"no_snippets\", 3)\n",
    "  retriever = get_vector_store_by_name(state[\"knowledge_base\"]).as_retriever(search_kwargs={\"k\": no_snippets})\n",
    "  snippets = retriever.get_relevant_documents(state[\"question\"])\n",
    "  return {\"snippets\": snippets}\n",
    "\n",
    "\n",
    "# Node #2 - generation\n",
    "def generate_answer(state: RAGState):\n",
    "  context = \"\\n\\n\".join(f\"text: {doc.page_content}\\n page: {doc.metadata['page_no']}\" for doc in state[\"snippets\"])\n",
    "  prompt = f\"Using the context below, answer the question and provide the information which page was used for creating the answer.\\n\\nQuestion: {state['question']}\\nContext: {context}\\nAnswer:\"\n",
    "  answer = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o-mini\").invoke(prompt).content\n",
    "  return {\"answer\": answer}\n",
    "\n",
    "\n",
    "# Build the state graph with sequential nodes\n",
    "graph_builder = StateGraph(RAGState).add_sequence([retrieve_snippets, generate_answer])\n",
    "graph_builder.add_edge(START, \"retrieve_snippets\")\n",
    "graph_builder.add_edge(\"generate_answer\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "graph.invoke({\"question\": \"Who was Calypso?\", \"knowledge_base\": \"ODDYSEY\"})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9MNk1Pj7_26",
    "outputId": "20c57c3d-ab19-44a3-ee81-f31c0f9fa5d8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Monitoring LLM application"
   ],
   "metadata": {
    "id": "m4TVopa5PzlE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instrumenting the application with monitoring\n",
    "from langfuse.callback import CallbackHandler\n",
    "import gradio as gr\n",
    "\n",
    "MY_USER_ID = \"mateusz\"\n",
    "\n",
    "langfuse_handler = CallbackHandler(trace_name=\"Workshops\", user_id=MY_USER_ID)\n",
    "\n",
    "def answer_oz_question(question: str, no_snippets: int, knowledge_base: str):\n",
    "    response = graph.invoke(\n",
    "        {'question': question, 'no_snippets': no_snippets, 'knowledge_base': knowledge_base},\n",
    "        config={\"callbacks\": [langfuse_handler]})\n",
    "    return response['answer']\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer_oz_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question\"),\n",
    "        gr.Slider(label=\"Number of snippets\", minimum=1, maximum=10, step=1, value=3),\n",
    "        gr.Dropdown(label=\"Knowledge Base\", choices=list(KNOWLEDGE_BASES.keys()), value=\"WIZARD_OF_OZ\")\n",
    "    ],\n",
    "    outputs=gr.TextArea(label=\"Answer\", interactive=False)\n",
    ")\n",
    "demo.launch(share=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "wAgPNhkx-vtw",
    "outputId": "685485f9-c901-4b8e-b4ea-14681a8628d3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now go to [https://cloud.langfuse.com](https://cloud.langfuse.com) and explore the traces."
   ],
   "metadata": {
    "id": "7ZGidseBPDZR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Evaluating prompts\n",
    "\n",
    "Demonstration of LLM-as-judge with Langfuse"
   ],
   "metadata": {
    "id": "4sUYfSeHgzsP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Prompt Management\n"
   ],
   "metadata": {
    "id": "ADC2fjSJ_XAN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a new prompt in langfuse prompt management [link](https://cloud.langfuse.com/project/cm8ymgiyi01osad069zauomkl/prompts). Use your name as a prefix so we can differentiate."
   ],
   "metadata": {
    "id": "eIvCN7TsQivn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()\n",
    "\n",
    "prompt = langfuse.get_prompt(\"mateuszpytel-prompt-test\")\n",
    "prompt.compile(question=\"<question_here>\", context=\"<context_here>\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "nTAG53W6_kny",
    "outputId": "0366c419-5fbc-4493-b457-66e58d229eb6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "* EXERCISE: Try modifying the application code above to use prompt from langfuse. Play around with different prompts, e.g. ask to answer in different language or in a funny way, like with rhymes for example. Observe how it impacts the score.\n",
   "metadata": {
    "id": "s0XUhtsXRvkj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Application deployment\n",
    "Demonstration on how to develop the app and deploy it to huggingfaces.\n",
    "\n",
    "1. Create a huggingface account\n",
    "2. Checkout the repo, go to `exercises/03-deployment`\n",
    "3. Run `gradio deploy` and follow the instructions"
   ],
   "metadata": {
    "id": "vjCG9Djxhoc9"
   }
  }
 ]
}
